
##### PREPARATION OF METHYLATION (or Single OMICS) FILE #####

## read in methylation file

meth = readRDS("meth_file.rds") 

## Read in phenotypes file 

phenos = read.csv("phenos.csv") 

## Match order of ids in both files 

ids = phenos$meth_id 
meth1 = meth[,match(ids,colnames(meth))] 
table(colnames(meth1) == phenos$meth_id) 

## Mean impute NAs - do this for every CpG site (transpose df so CpGs are columns) 

tmeth1 = t(meth1) 
tmeth1 = as.data.frame(tmeth1) 
library(imputeTS) 
tmeth1 = na_mean(tmeth1) 

## Scale every row - should be CpGs as rows (transpose df) 

tmeth1 = t(tmeth1) 
tmeth1 = as.data.frame(tmeth1) 

tmeth1 = apply(tmeth1, 1, scale)

## Final file should be individuals as columns and ~800k probes as rows 

write.table(x = t(tmeth1), "Single_Omics.csv", sep = ",", row.names = F, col.names = F, quote = F) 

## Keep CpG Identifiers as row.names are deleted from file for BayesR+ 

names1 <- colnames(tmeth1) 
saveRDS(names1, "CpG_Names_BayesR.rds") 

###### To scale up to two omics type, prior to processing, rbind/cbind file of CpGs and SNPs so that for every individual their value for each CpG and SNP is shown. Apply same processing principles. 

## PREPARATION OF COVARIATES FILE 

cov <- read.csv("Some_Covariates_File.csv")

## Scale all of the columns of interest and make sure no NA are present
## Final structure should be columns as covariates and rows as individuals matched in order to that of other files  

write.table(x = as.matrix(cov),file = "Some_Scaled_Covariates_File.csv" ,quote = F, sep = ",", row.names = F, col.names = F)



## PREPARATION OF GROUPS FILE

## This is a .txt file 
## First column shows sites - in my example, CpGs and SNPs
## Second column shows group - either 0 for CpGs or 1 for SNPs
## Order of CpGs and SNPs matches the order of the sites in the row.names of Omics.csv file  



############ RUNNING BAYESR in TERMINAL ######################################
## Example Code - EWAS 
## For GWAS, change mixtures to "0.01,0.1" and combined model GWAS:EWAS change to "0.01,0.1,0.2;0.001,0.01,0.1" with groups.txt file

cd filepath
export LD_LIBRARY_PATH=/opt/gcc/lib64
FILES=(*) 
x=1
x1=`expr $x - 1`
y=`expr 17 \* $x1`
z=`expr 17 \* $x`
for i in "${FILES[@]:$y:17}"

do  A=$( echo $i | cut -d"." -f1)

../../../BayesRRcmd/src/brr --data-file ../Epigenetics/Run1/BayesR_Meth.csv --pheno P07355.csvphen --analysis-type preprocess --fixed_effects ../Covariates_File.csv --fixedEffectNumber 5  --thread 12 --thread-spawned 12 --marker-cache --seed 1

../../../BayesRRcmd/src/brr --data-file ../Epigenetics/Run1/BayesR_Meth.csv --pheno P07355.csvphen --analysis-type ppbayes --chain-length 10000 --burn-in 5000 --thin 5 --fixed_effects ../Covariates_File.csv --fixedEffectNumber 5  --S "0.001, 0.01, 0.1" --mcmc-samples ../EWAS_Phenotypes_Output/P07355.csv --thread 12 --thread-spawned 12 --marker-cache --seed 1

done 


####### EXTRACT SIGMA, BETA AND COMP FILES IN TERMINAL ###############



## Set up loop for any output file with a *.csv extension 

cd /filepath_to_BayesR_Outputs/

for i in *.csv

do

## Extract columns with sigma name - sigmaG (variance explained) and sigmaE (error term - unexplained proportion of variance)  

sigma1=$(head -1 $i | sed 's/,/\n/g' | cat -n | grep -n "sigma" | cut -f 1 |  sed 's/:/\n/g' | awk 'NR==1')
sigma2=$(head -1 $i | sed 's/,/\n/g' | cat -n | grep -n "sigma" | cut -f 1 |  sed 's/:/\n/g' | awk 'END{print $NF}')

## Create variable name 

A=$( echo $i | cut -d"/" -f3)
B=$( echo $A | cut -d_ -f1)

cat $i | cut -d ',' -f $sigma1-$sigma2 > ../../Sigma/$B.csv


## Extract columns with beta name - goes from first to last column 

beta1=$(head -1 $i | sed 's/,/\n/g' | cat -n | grep -n "beta" | cut -f 1 | sed 's/:/\n/g' | awk 'NR==1')
beta2=$(head -1 $i | sed 's/,/\n/g' | cat -n | grep -n "beta" | cut -f 1 | sed 's/:/\n/g' | awk 'END{print $NF}')

A=$( echo $i | cut -d"/" -f3)
B=$( echo $A | cut -d_ -f1)

cat $i | cut -d ',' -f $beta1-$beta2 > ../../Beta/$B.csv

## Extract columns with components name - goes from first to last column; within each iteration, tells you which group marker was assigned to e.g. small, medium, large effect 

comp1=$(head -1 $i | sed 's/,/\n/g' | cat -n | grep -n "comp" | cut -f 1 | sed 's/:/\n/g' | awk 'NR==1')
comp2=$(head -1 $i | sed 's/,/\n/g' | cat -n | grep -n "comp" | cut -f 1 | sed 's/:/\n/g' | awk 'END{print $NF}')

A=$( echo $i | cut -d"/" -f3)
B=$( echo $A | cut -d_ -f1)

cat $i | cut -d ',' -f $comp1-$comp2 > ../../Comp/$B.csv

done



##### CALCULATE POSTERIOR INCLUSION PROBABILITIES ##################

setwd("Some_path/to/PIP") 
loop = list.files("../Comp/", pattern = ".csv") 
install.packages("data.table")
library(data.table) 
names = readRDS("../CpG_Names.rds") 
for(i in loop){ 
  comp <- fread(paste("../Comp/", i, sep=""))  
  comp<-as.data.frame(comp) 

  pip <- lapply(comp,function(x){length(x[which(x>0)])/length(x)})
  pip <- as.data.frame(reshape2::melt(unlist(pip)))
  pip <- setDT(pip, keep.rownames = TRUE) 
  names(pip) <- c("Marker", "PIP") 
  pip$Marker <- names
  A <-gsub(".csv*","",i)
  pip$Biomarker <- A 
  
write.csv(pip, file = paste(A, "_pip.csv", sep = ""), row.names = F) 
} 


## Find All CpGs that had PIP > 0.95

L <- list.files(".", ".csv")

pip_files <- lapply(L, read.csv, header = T) 
names <- as.character(L) 
names <- gsub("_.*", "", names) 
names(pip_files) <- names 
pip_files = Map(cbind, pip_files, "Biomarker"=names) 

pip_top <- do.call(rbind, lapply(pip_files, function(x)x[x$PIP > 0.95,]))



#### GENERATE BAYESR SUMMARY OUTPUTS ##################

setwd("Path_to_Files/") 

loop = list.files("Sigma/", pattern = ".csv") 

 ## Step 1 - Calculate Mean Variance explained by all probes and credible intervals

names = readRDS("../CpG_Names_BayesR.rds")  
for(i in loop){  

output = matrix(nrow =1, ncol = 1) 
output <- as.data.frame(output) 
names(output)[1] <- "Biomarker" 

sigma <- read.csv(paste("Sigma/", i, sep=""))  
output$Genetic_Mean_Variance_Explained = mean(sigma[,3]/rowSums(sigma[,1:3]),na.rm = T)
output$Epigenetic_Mean_Variance_Explained = mean(sigma[,2]/rowSums(sigma[,1:3]),na.rm = T)
output$Combined_Mean_Variance_Explained = mean((sigma[,2]+sigma[,3])/rowSums(sigma[,1:3]),na.rm = T)
output$Genetic_Low_CI = quantile(sigma[,3]/rowSums(sigma[,1:3]),na.rm = T, prob =0.025)
output$Genetic_High_CI = quantile(sigma[,3]/rowSums(sigma[,1:3]),na.rm = T, prob =0.975)
output$Epigenetic_Low_CI = quantile(sigma[,2]/rowSums(sigma[,1:3]),na.rm = T, prob =0.025)
output$Epigenetic_High_CI = quantile(sigma[,2]/rowSums(sigma[,1:3]),na.rm = T, prob =0.975)
output$Combined_Low_CI = quantile((sigma[,2]+sigma[,3])/rowSums(sigma[,1:3]),na.rm = T, prob =0.025)
output$Combined_High_CI = quantile((sigma[,2] + sigma[,3])/rowSums(sigma[,1:3]),na.rm = T, prob =0.975)

## Step 2 - Calculate the proportion of variance that is attributable to small, medium and large effects - 1,2,3
 
  betas <- fread(paste("Beta/", i, sep=""))
  betas = as.data.frame(betas)
  comp <- fread(paste("Comp/", i, sep=""))  
  comp = as.data.frame(comp) 
  names(comp) <- names
  names(betas) <- names
  list = apply(comp,1,function(x)which(!x%in% c(1,2,3)))  
  
  
  x <- as.matrix(0, ncol = 1, nrow = 1000) 
  x <-as.data.frame(x) 
  for(i in 1:1000){ 
 x[[i]]<-length(list[[i]]) == 459309 ##### NUMBER OF PROBES/MARKERS STUDIED HERE ###### 
 } 
 
 if(length(which(x[1,] %in% "TRUE")) > 0){ 
   comp = comp[-which(x %in% "TRUE"),] 
 } else { 
 comp = comp} 
 
 
 t <- vector() 
list = apply(comp,1,function(x)which(x %in% 1)) 
 for(i in 1:1000){ 
 t[[i]] <- length(list[[i]]) > 0 
 } 
 ind_true = which(t %in% "TRUE")
 ind_false = which(t %in% "FALSE")
 list_true = list[ind_true]
 list_false = list[ind_false] 
 n = length(list_true) 
  m1_true <- matrix(0, ncol = 1, nrow = n)
 m1_true <- as.data.frame(m1_true) 
 m1_true$ind <- ind_true
x<-vector()
for(j in m1_true$ind){ 
 x[j] <- sum((betas[j,list[[j]]])^2) 
 } 
m1= as.data.frame(x) 
m1$x[is.na(m1$x)] <- 0 
names(m1) <- "Variance_Small_Effects" 
 
 list = apply(comp,1,function(x)which(x %in% 2)) 
 for(i in 1:1000){ 
 t[[i]] <- length(list[[i]]) > 0 
 } 
 ind_true = which(t %in% "TRUE")
 ind_false = which(t %in% "FALSE")
 list_true = list[ind_true]
 list_false = list[ind_false] 
 n = length(list_true) 
m2_true <- matrix(0, ncol = 1, nrow = n)
 m2_true <- as.data.frame(m2_true) 
 m2_true$ind <- ind_true
x<-vector()
for(j in m2_true$ind){ 
 x[j] <- sum((betas[j,list[[j]]])^2) 
 } 
m2= as.data.frame(x) 
m2$x[is.na(m2$x)] <- 0 
names(m2) <- "Variance_Medium_Effects"
  
list = apply(comp,1,function(x)which(x %in% 3)) 
 for(i in 1:1000){ 
 t[[i]] <- length(list[[i]]) > 0 
 } 
 ind_true = which(t %in% "TRUE")
 ind_false = which(t %in% "FALSE")
 list_true = list[ind_true]
 list_false = list[ind_false] 
 n = length(list_true) 
  m3_true <- matrix(0, ncol = 1, nrow = n)
 m3_true <- as.data.frame(m3_true) 
 m3_true$ind <- ind_true
x<-vector()
for(j in m3_true$ind){ 
 x[j] <- sum((betas[j,list[[j]]])^2) 
 } 
m3= as.data.frame(x) 
m3$x[is.na(m3$x)] <- 0 
names(m3) <- "Variance_Large_Effects"
  
m1$num <- row.names(m1) 
m2$num <- row.names(m2) 
m3$num <- row.names(m3) 
all = merge(m1, m2, by = "num", all = T) 
var = merge(all, m3, by = "num", all = T) 
var[is.na(var)] <- 0 
var$num <- NULL
var$Total_Variance <- var[,1] + var[,2] + var[,3]
var$Proportion_Small_Effects <- var[,1]/var[,4]
var$Proportion_Medium_Effects <- var[,2]/var[,4]
var$Proportion_Large_Effects <- var[,3]/var[,4]
output$Proportion_Small_Effects <- mean(var$Proportion_Small_Effects) 
output$Proportion_Medium_Effects <- mean(var$Proportion_Medium_Effects) 
output$Proportion_Large_Effects <- mean(var$Proportion_Large_Effects) 

output$Biomarker <- A

write.csv(output, file = paste("Meth_Outputs/", A, "_output.csv", sep = ""), row.names = F) 

}

## Combine the output files into one larger file with all biomarkers 

files  <- list.files("Meth_Outputs/", ".csv")
tables <- lapply(paste("Meth_Outputs/",files,sep = ""), read.csv, header = TRUE)
combined.df <- do.call(rbind , tables)
write.csv(combined.df, "Summary_Bayes_R.csv", row.names = F) 

